{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xcOJcaApdauY"
   },
   "source": [
    "# Postraining Optimization Toolkit. Advanced topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fYlPqVeXTPDw"
   },
   "source": [
    "This notebook considers how to support custom model quantization via POT with Accuracy Checker backend which is a fundamental part of POT responsible for data reading, pre- & post- processing, inference launching and metrics collection.\n",
    "\n",
    "Accuracy Checker is the tool for models accuracy validation. It has a lot of predefined configuration options for dataset conversion, preprocessing, postprocessing and metric evaluation. However sometimes its capabilities is not enough for running particular model.\n",
    "\n",
    "Lets duscuss, what should we do in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequsites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'open_model_zoo'...\n",
      "remote: Enumerating objects: 39, done.\u001b[K\n",
      "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
      "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
      "remote: Total 38298 (delta 13), reused 7 (delta 3), pack-reused 38259\u001b[K\n",
      "Receiving objects: 100% (38298/38298), 81.41 MiB | 19.07 MiB/s, done.\n",
      "Resolving deltas: 100% (25455/25455), done.\n",
      "Checking out files: 100% (1502/1502), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/opencv/open_model_zoo.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_PITDgseUtTx"
   },
   "source": [
    "## First of all, a couple of words about Accuracy Checker architecture.\n",
    "\n",
    "Accuracy Checker has a  modular structure. This tool is very flexible and easy to extend with new components.\n",
    "\n",
    "The common aproach of adding new functionality includes the following steps:\n",
    "1. Create a new class for the component. \n",
    "2. The parent class of the new component should be basic abstract class for all objects of extended module.\n",
    "3. Implement all abstract methods for a base class in the new component.\n",
    "4. Define the name for configuration in the `__provider__` field.\n",
    "5. Optionally, if component should have configurable parameters, you need to specify them in `parameters()` method.\n",
    "6. Finally, you need to register new functionality inside __init__.py of extendable module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mudCcygoXX_Q"
   },
   "source": [
    "## Case 1: Emotion Recognition\n",
    "\n",
    "Now we'll go through all these steps on the example of [Emotion Recognition model](https://github.com/onnx/models/tree/master/vision/body_analysis/emotion_ferplus) from onnx model zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5wifB_gmYHHP"
   },
   "source": [
    "### Step 0 - Analyze the Model\n",
    "At the begining, we need to understand which component should we modify.\n",
    "It's not enough to just convert the model to IR for that reason - we need to get to know the model better. \n",
    "\n",
    "During the analysis you should find answers to the following questions:\n",
    "\n",
    "*   What is the model use case? For which purpose does it used?\n",
    "*   Which dataset should I use for evaluation?\n",
    "*   Which preprocessing steps should be preformed?\n",
    "*   How to retrive results from the model and postprocess them?\n",
    "*   Which metric should be used?\n",
    "\n",
    "Let's come back to our emotion recognition model. We can find all needed information from readme - https://github.com/onnx/models/blob/master/vision/body_analysis/emotion_ferplus/README.md\n",
    "\n",
    "*   What is the model use case? For which purpose does it used? - __emotions_recognition__, __classification__\n",
    "*   Which dataset should I use for evaluation? **FER+ stored in csv format** \n",
    "*   Which preprocessing steps should be preformed? **resize image with antialias interpolation to 64x64 size**\n",
    "*   How to retrive results from the model and postprocess them? **model returns tensor with shape [N, 8], where N - number images in batch size. It contains probabilities in raw logits format. Possibly softmax operation should be applied.** \n",
    "*   Which metric should be used? **emotion recognition is particular case of classification task, so classification accuracy can be suitable for model evaluation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_6JqZv3_tFXQ"
   },
   "source": [
    "### Step 1. Support new dataset.\n",
    "\n",
    "Emotion recognition model was trained on FER+ dataset. It can be download from here - https://www.kaggle.com/deadskull7/fer2013\n",
    "\n",
    "The dataset represented as csv table with 3 fields:\n",
    "* *emotion* - ground truth label\n",
    "* *pixels* - array of pixel intensity which represent gray scale image in size 48x48. Also it make sence to say that they given in flatten format row by row.\n",
    "* *usage* - split of dataset which image belongs (`Training`, `PublicTest`). For validation and quantization we only interested in validation part, so we should implement image filtering by this field.\n",
    "\n",
    "Keeping in mind all these details, lets implement converter for dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g4mvPn2tBnI6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting advanced_materials/fer_plus_converter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile advanced_materials/fer_plus_converter.py\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "# classes which represent configuration parameters in the code\n",
    "from ..config import PathField, BoolField\n",
    "# data type which was generated during annotation conversion\n",
    "from ..representation import ClassificationAnnotation\n",
    "from ..utils import read_csv\n",
    "\n",
    "from .format_converter import BaseFormatConverter, ConverterReturn\n",
    "\n",
    "\n",
    "\n",
    "class FERPlusFormatConverter(BaseFormatConverter):\n",
    "    \"\"\"\n",
    "    FER+ dataset converter. All annotation converters should be derived from BaseFormatConverter class.\n",
    "    Annotation data for conversion can be found here https://www.kaggle.com/deadskull7/fer2013\n",
    "    \"\"\"\n",
    "\n",
    "    # register name for this converter\n",
    "    # this name will be used for converter class look up\n",
    "    __provider__ = 'fer_plus'\n",
    "    # specify a hint about generated data type\n",
    "    annotation_types = (ClassificationAnnotation, )\n",
    "\n",
    "    @classmethod\n",
    "    def parameters(cls):\n",
    "        \"\"\"\n",
    "        describe config parsing template for this converter\n",
    "        :return: dictionary, where config fields used as keys and helpers for config parsing as values.\n",
    "        \"\"\"\n",
    "        # get basic parameters from parent class\n",
    "        configuration_parameters = super().parameters()\n",
    "        # update them with new\n",
    "        configuration_parameters.update({\n",
    "            'annotation_file': PathField(description=\"Path to csv file which contain dataset.\"),\n",
    "            'convert_images': BoolField(\n",
    "                optional=True,\n",
    "                default=False,\n",
    "                description=\"Allows to convert images from pickle file to user specified directory.\"\n",
    "            ),\n",
    "            'converted_images_dir': PathField(\n",
    "                optional=True, is_directory=True, check_exists=False, description=\"Path to converted images location.\"\n",
    "            )\n",
    "        })\n",
    "\n",
    "        return configuration_parameters\n",
    "\n",
    "    def configure(self):\n",
    "        \"\"\"\n",
    "        This method is responsible for obtaining the necessary parameters\n",
    "        for converting from the command line or config.\n",
    "        \"\"\"\n",
    "        self.csv_file = self.get_value_from_config('annotation_file')\n",
    "        self.converted_images_dir = self.get_value_from_config('converted_images_dir')\n",
    "        self.convert_images = self.get_value_from_config('convert_images')\n",
    "        if self.convert_images and not self.converted_images_dir:\n",
    "            self.converted_images_dir = self.csv_file.parent / 'converted_images'\n",
    "            if not self.converted_images_dir.exists():\n",
    "                self.converted_images_dir.mkdir(parents=True)\n",
    "\n",
    "        if self.convert_images and Image is None:\n",
    "            raise ValueError(\n",
    "                \"conversion mnist images requires Pillow installation, please install it before usage\"\n",
    "            )\n",
    "\n",
    "    def convert(self, check_content=False, progress_callback=None, progress_interval=100, **kwargs):\n",
    "        \"\"\"\n",
    "        This method is executed automatically when convert.py is started.\n",
    "        All arguments are automatically got from command line arguments or config file in method configure\n",
    "\n",
    "        Returns:\n",
    "            annotations: list of annotation representation objects.\n",
    "            meta: dictionary with additional dataset level metadata.\n",
    "            content errors: service field for errors handling\n",
    "        \"\"\"\n",
    "        annotations = []\n",
    "        # read original dataset annotation\n",
    "        annotation_table = read_csv(self.csv_file)\n",
    "        # process object by object\n",
    "        for index, annotation in enumerate(annotation_table):\n",
    "            # ignore data not from testing subset\n",
    "            if annotation['Usage'] != 'PublicTest':\n",
    "                continue\n",
    "            # identifier is unique name of data in the dataset. For images usually file name used\n",
    "            identifier = '{}.png'.format(index)\n",
    "            # getting label\n",
    "            label = int(annotation['emotion'])\n",
    "            # since our annotation contains pixels intensity inside the table,\n",
    "            # we need to get images from it for more convenient usage.\n",
    "            # convert images once, we can turn off this flag in the config and use pregenerated images\n",
    "            if self.convert_images:\n",
    "                pixels_array = [int(y) for y in annotation['pixels'].split()]\n",
    "                pixels = np.array(pixels_array).reshape(48, 48)\n",
    "                image = Image.fromarray(pixels.astype(np.uint8))\n",
    "                image = image.convert(\"L\")\n",
    "                image.save(str(self.converted_images_dir / identifier))\n",
    "            # create a new instance of the annotation representation\n",
    "            # different representations can have different set of parameters required for metric calculation\n",
    "            # for ClassificationAnnotation, identifier and label used.\n",
    "            annotations.append(ClassificationAnnotation(identifier, label))\n",
    "\n",
    "        # metadata contains specific info about dataset which can help during the evaluation\n",
    "        # (e.g. mapping of labels, has background label in the dataset or not)\n",
    "        # for some task where additional info is not required it can be left empty of None\n",
    "        meta = self.get_meta()\n",
    "\n",
    "        # finally, this method should return the named tuple with fields annotations, meta and content errors\n",
    "        return ConverterReturn(annotations, meta, None)\n",
    "\n",
    "\n",
    "    def get_meta(self):\n",
    "        # use original lables from dataset\n",
    "        emotion_table = {'neutral': 0, 'happiness': 1, 'surprise': 2, 'sadness': 3, 'anger': 4, 'disgust': 5, 'fear': 6,\n",
    "                         'contempt': 7}\n",
    "        # inside Accuracy Checker we use class_id as label, so label map should be represented as class_id: class_name\n",
    "        label_map = {v: k  for v, k in emotion_table.items()}\n",
    "\n",
    "        # dataset meta should be represented like dictionary, label_map key used for storing label mapping\n",
    "        return {'label_map': label_map}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WMWH9wUzvWkE"
   },
   "source": [
    "Now, lets integrate it to Accuracy Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "01ckAKATvVv_"
   },
   "outputs": [],
   "source": [
    "!cp advanced_materials/fer_plus_converter.py open_model_zoo/tools/accuracy_checker/accuracy_checker/annotation_converters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t4IG_2uhv8sv"
   },
   "source": [
    "New class registration involves import new functionality inside `__init__.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jVXNK7hcvxsc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting open_model_zoo/tools/accuracy_checker/accuracy_checker/annotation_converters/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile open_model_zoo/tools/accuracy_checker/accuracy_checker/annotation_converters/__init__.py\n",
    "\"\"\"\n",
    "Copyright (c) 2019 Intel Corporation\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\"\n",
    "\n",
    "from .format_converter import BaseFormatConverter\n",
    "from .convert import make_subset, save_annotation, analyze_dataset\n",
    "from .market1501 import Market1501Converter\n",
    "from .veri776 import VeRi776Converter\n",
    "from .mars import MARSConverter\n",
    "from .pascal_voc import PascalVOCDetectionConverter\n",
    "from .sample_converter import SampleConverter\n",
    "from .wider import WiderFormatConverter\n",
    "from .detection_opencv_storage import DetectionOpenCVStorageFormatConverter\n",
    "from .lfw import LFWConverter\n",
    "from .vgg_face_regression import VGGFaceRegressionConverter\n",
    "from .super_resolution_converter import SRConverter, SRMultiFrameConverter, MultiTargetSuperResolutionConverter\n",
    "from .imagenet import ImageNetFormatConverter\n",
    "from .icdar import ICDAR13RecognitionDatasetConverter, ICDAR15DetectionDatasetConverter\n",
    "from .kondate_nakayosi import KondateNakayosiRecognitionDatasetConverter\n",
    "from .ms_coco import MSCocoDetectionConverter, MSCocoKeypointsConverter, MSCocoSingleKeypointsConverter\n",
    "from .cityscapes import CityscapesConverter\n",
    "from .ncf_converter import MovieLensConverter\n",
    "from .brats import BratsConverter, BratsNumpyConverter\n",
    "from .oar3d import OAR3DTilingConverter\n",
    "from .cifar import CifarFormatConverter\n",
    "from .mnist import MNISTCSVFormatConverter\n",
    "from .wmt import WMTConverter\n",
    "from .common_semantic_segmentation import CommonSegmentationConverter\n",
    "from .camvid import CamVidConverter\n",
    "from .lpr import LPRConverter\n",
    "from .image_retrieval import ImageRetrievalConverter\n",
    "from .cvat_object_detection import CVATObjectDetectionConverter\n",
    "from .cvat_attributes_recognition import CVATAttributesRecognitionConverter\n",
    "from .cvat_age_gender_recognition import CVATAgeGenderRecognitionConverter\n",
    "from .cvat_facial_landmarks import CVATFacialLandmarksRecognitionConverter\n",
    "from .cvat_text_recognition import CVATTextRecognitionConverter\n",
    "from .cvat_multilabel_recognition import CVATMultilabelAttributesRecognitionConverter\n",
    "from .cvat_human_pose import CVATPoseEstimationConverter\n",
    "from .cvat_person_detection_action_recognition import CVATPersonDetectionActionRecognitionConverter\n",
    "from .mrlEyes_2018_01 import mrlEyes_2018_01_Converter\n",
    "from .squad import SQUADConverter\n",
    "from .text_classification import (\n",
    "    XNLIDatasetConverter,\n",
    "    BertXNLITFRecordConverter,\n",
    "    IMDBConverter,\n",
    "    MRPCConverter,\n",
    "    CoLAConverter\n",
    ")\n",
    "from .cmu_panoptic import CmuPanopticKeypointsConverter\n",
    "from .action_recognition import ActionRecognitionConverter\n",
    "from .ms_asl_continuous import MSASLContiniousConverter\n",
    "\n",
    "from .monocular_depth_perception import ReDWebDatasetConverter\n",
    "\n",
    "from .fashion_mnist import FashionMnistConverter\n",
    "from .inpainting import InpaintingConverter\n",
    "from .style_transfer import StyleTransferConverter\n",
    "from .wikitext2raw import Wikitext2RawConverter\n",
    "\n",
    "from  .image_processing import ImageProcessingConverter\n",
    "\n",
    "from .aflw2000_3d import AFLW20003DConverter\n",
    "\n",
    "from .fer_plus_converter import FERPlusFormatConverter\n",
    "\n",
    "__all__ = [\n",
    "    'BaseFormatConverter',\n",
    "    'make_subset',\n",
    "    'save_annotation',\n",
    "    'analyze_dataset',\n",
    "\n",
    "    'ImageNetFormatConverter',\n",
    "    'Market1501Converter',\n",
    "    'VeRi776Converter',\n",
    "    'SampleConverter',\n",
    "    'PascalVOCDetectionConverter',\n",
    "    'WiderFormatConverter',\n",
    "    'MARSConverter',\n",
    "    'DetectionOpenCVStorageFormatConverter',\n",
    "    'LFWConverter',\n",
    "    'VGGFaceRegressionConverter',\n",
    "    'SRConverter',\n",
    "    'SRMultiFrameConverter',\n",
    "    'MultiTargetSuperResolutionConverter',\n",
    "    'ICDAR13RecognitionDatasetConverter',\n",
    "    'ICDAR15DetectionDatasetConverter',\n",
    "    'KondateNakayosiRecognitionDatasetConverter',\n",
    "    'MSCocoKeypointsConverter',\n",
    "    'MSCocoSingleKeypointsConverter',\n",
    "    'MSCocoDetectionConverter',\n",
    "    'CityscapesConverter',\n",
    "    'MovieLensConverter',\n",
    "    'BratsConverter',\n",
    "    'BratsNumpyConverter',\n",
    "    'OAR3DTilingConverter',\n",
    "    'CifarFormatConverter',\n",
    "    'MNISTCSVFormatConverter',\n",
    "    'WMTConverter',\n",
    "    'CommonSegmentationConverter',\n",
    "    'CamVidConverter',\n",
    "    'LPRConverter',\n",
    "    'ImageRetrievalConverter',\n",
    "    'CVATObjectDetectionConverter',\n",
    "    'CVATAttributesRecognitionConverter',\n",
    "    'CVATAgeGenderRecognitionConverter',\n",
    "    'CVATFacialLandmarksRecognitionConverter',\n",
    "    'CVATTextRecognitionConverter',\n",
    "    'CVATMultilabelAttributesRecognitionConverter',\n",
    "    'CVATPoseEstimationConverter',\n",
    "    'CVATPersonDetectionActionRecognitionConverter',\n",
    "    'SQUADConverter',\n",
    "    'XNLIDatasetConverter',\n",
    "    'BertXNLITFRecordConverter',\n",
    "    'IMDBConverter',\n",
    "    'MRPCConverter',\n",
    "    'CoLAConverter',\n",
    "    'CmuPanopticKeypointsConverter',\n",
    "    'ActionRecognitionConverter',\n",
    "    'MSASLContiniousConverter',\n",
    "    'ReDWebDatasetConverter',\n",
    "    'FashionMnistConverter',\n",
    "    'InpaintingConverter',\n",
    "    'mrlEyes_2018_01_Converter',\n",
    "    'StyleTransferConverter',\n",
    "    'Wikitext2RawConverter',\n",
    "    'ImageProcessingConverter',\n",
    "    'AFLW20003DConverter',\n",
    "    'FERPlusFormatConverter',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ItPrGjcfxQ0C"
   },
   "source": [
    "### Step 2. Implement the preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ALXzoFuRxW0S"
   },
   "source": [
    "Adding new preprocessor looks simmilar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8evagMZQv5dV"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "attempted relative import beyond top-level package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-581be02eb675>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# base class for all preprocessors is Preprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# helpers for configuration parsing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNumberField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: attempted relative import beyond top-level package"
     ]
    }
   ],
   "source": [
    "# %load advanced_materials/emotion_recognition_preprocessing.py\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# base class for all preprocessors is Preprocessor\n",
    "from ..preprocessor import Preprocessor\n",
    "# helpers for configuration parsing\n",
    "from ..config import NumberField\n",
    "\n",
    "\n",
    "class EmotionRecognitionResize(Preprocessor):\n",
    "    # name of preprocessor for configuration\n",
    "    __provider__ = 'emotion_recognition_preprocessing'\n",
    "\n",
    "    # definition of important configuration parameters\n",
    "    # for image resizing we need to know target size\n",
    "    @classmethod\n",
    "    def parameters(cls):\n",
    "        parameters = super().parameters()\n",
    "        parameters.update({\n",
    "            'size': NumberField(\n",
    "                value_type=int, optional=False, min_value=1, description=\"Destination sizes for both dimensions.\"\n",
    "            ),\n",
    "        })\n",
    "\n",
    "        return parameters\n",
    "\n",
    "    def configure(self):\n",
    "        # getting parameters from config\n",
    "        self.size = self.get_value_from_config('size')\n",
    "\n",
    "    def process(self, image, annotation_meta=None):\n",
    "        \"\"\"\n",
    "        Preprocessor realization function, which will be called for each image in the input dataset\n",
    "        :param image: DataRepresentation entry which include read image and related metadata for it.\n",
    "        :param annotation_meta: Dictionary with info from  annotation.\n",
    "                                optional, used in case when we need to use or update some info about image\n",
    "        :return: DataRepresentation with updated image\n",
    "        \"\"\"\n",
    "        # image dasta stored inside DataRepresentation in data field\n",
    "        data = image.data\n",
    "        # internally we work with numpy arrays, so we need to convert it to pillow image object for resizing\n",
    "        resized_data = Image.fromarray(data).resize((self.size, self.size), Image.ANTIALIAS)\n",
    "        # return back data to numpy array\n",
    "        data = np.array(resized_data)\n",
    "        # expand dims for gray scale image\n",
    "        if len(data.shape) == 2:\n",
    "            data = np.expand_dims(data, axis=-1)\n",
    "        image.data = data\n",
    "        # returns updated DataRepresentation\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O-oBror5yZv0"
   },
   "outputs": [],
   "source": [
    "!cp advanced_materials/emotion_recognition_preprocessing.py open_model_zoo/tools/accuracy_checker/accuracy_checker/preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "78jPyETzyRhN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting open_model_zoo/tools/accuracy_checker/accuracy_checker/preprocessor/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile open_model_zoo/tools/accuracy_checker/accuracy_checker/preprocessor/__init__.py\n",
    "\"\"\"\n",
    "Copyright (c) 2019 Intel Corporation\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\"\n",
    "\n",
    "from .preprocessing_executor import PreprocessingExecutor\n",
    "from .preprocessor import Preprocessor\n",
    "from .audio_preprocessing import ResampleAudio, ClipAudio, NormalizeAudio\n",
    "from .color_space_conversion import (\n",
    "    BgrToRgb, RgbToBgr, BgrToGray, RgbToGray, TfConvertImageDType, SelectInputChannel, BGR2YUVConverter\n",
    ")\n",
    "from .normalization import Normalize, Normalize3d\n",
    "from .geometric_transformations import (\n",
    "    GeometricOperationMetadata,\n",
    "    Flip,\n",
    "    Crop,\n",
    "    CropRect,\n",
    "    ExtendAroundRect,\n",
    "    PointAligner,\n",
    "    Tiling,\n",
    "    Crop3D,\n",
    "    TransformedCropWithAutoScale,\n",
    "    ImagePyramid,\n",
    "    FaceDetectionImagePyramid,\n",
    "    WarpAffine,\n",
    "    FacePatch\n",
    ")\n",
    "from .resize import Resize, AutoResize\n",
    "from .nlp_preprocessors import DecodeByVocabulary, PadWithEOS\n",
    "from .centernet_preprocessing import CenterNetAffineTransformation\n",
    "from .brats_preprocessing import Resize3D, NormalizeBrats, CropBraTS, SwapModalitiesBrats\n",
    "from .inpainting_preprocessor import FreeFormMask, RectMask, CustomMask\n",
    "\n",
    "from .emotion_recognition_preprocessing import EmotionRecognitionResize\n",
    "\n",
    "__all__ = [\n",
    "    'PreprocessingExecutor',\n",
    "\n",
    "    'Preprocessor',\n",
    "    'GeometricOperationMetadata',\n",
    "\n",
    "    'ResampleAudio',\n",
    "    'ClipAudio',\n",
    "    'NormalizeAudio',\n",
    "\n",
    "    'Resize',\n",
    "    'Resize3D',\n",
    "    'AutoResize',\n",
    "    'Flip',\n",
    "    'Crop',\n",
    "    'CropRect',\n",
    "    'ExtendAroundRect',\n",
    "    'PointAligner',\n",
    "    'Tiling',\n",
    "    'Crop3D',\n",
    "    'CropBraTS',\n",
    "    'TransformedCropWithAutoScale',\n",
    "    'ImagePyramid',\n",
    "    'FaceDetectionImagePyramid',\n",
    "    'WarpAffine',\n",
    "    'FacePatch',\n",
    "\n",
    "    'BgrToGray',\n",
    "    'BgrToRgb',\n",
    "    'RgbToGray',\n",
    "    'RgbToBgr',\n",
    "    'BGR2YUVConverter',\n",
    "    'TfConvertImageDType',\n",
    "    'SelectInputChannel',\n",
    "\n",
    "    'Normalize3d',\n",
    "    'Normalize',\n",
    "    'NormalizeBrats',\n",
    "\n",
    "    'SwapModalitiesBrats',\n",
    "\n",
    "    'DecodeByVocabulary',\n",
    "    'PadWithEOS',\n",
    "\n",
    "    'CenterNetAffineTransformation',\n",
    "\n",
    "    'FreeFormMask',\n",
    "    'RectMask',\n",
    "    'CustomMask',   \n",
    "    'EmotionRecognitionResize'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Twh4CxaEyyq-"
   },
   "source": [
    "### Step 3: implement output parsing and postprocessing.\n",
    "Adapter responsibility is to convert raw model output to  the suitable for metric calculation format. Inside accuracy checker, there is also postprocessor entity. The main difference between adapter and postprocessor is that postprocessor is an optional step in predicted data preparation - filtering, NMS, casting to integer, clipping and so on. Also postprocessor can work with annotation content in some cases (e.g. in popular datasets for semantic segmentation task, annotation represented as png mask where each class represented by specific color, for metric evaluation, we need to convert color to class ids).\n",
    "\n",
    "In our task, we do not need additional postprocessing steps, so an adapter will be enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QTak0h5g1G0g"
   },
   "outputs": [],
   "source": [
    "!cp advanced_materials/emotion_recognition_adapter.py open_model_zoo/tools/accuracy_checker/accuracy_checker/adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bAMZtKd20r0l"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "attempted relative import beyond top-level package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-73b0a3902c46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# %load advanced_materials/emotion_recognition_adapter.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# base class for all adapters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# output format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassificationPrediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: attempted relative import beyond top-level package"
     ]
    }
   ],
   "source": [
    "# %load advanced_materials/emotion_recognition_adapter.py\n",
    "# base class for all adapters\n",
    "from ..adapters import Adapter\n",
    "# output format\n",
    "from ..representation import ClassificationPrediction\n",
    "\n",
    "\n",
    "class EmotionRecognitionAdapter(Adapter):\n",
    "    \"\"\"\n",
    "    Class for converting output of emotion recognition model to ClassificationPrediction representation\n",
    "    \"\"\"\n",
    "    # new adapter name in the config\n",
    "    __provider__ = 'emotion_recognition'\n",
    "    # like other components adapter might have parameters for configuration, but in our case they are not used\n",
    "    # we can use default implementation of these parameters\n",
    "\n",
    "    def process(self, raw, identifiers=None, frame_meta=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            identifiers: list of input data identifiers\n",
    "            raw: output of model\n",
    "            frame_meta: list of meta information about each frame\n",
    "        Returns:\n",
    "            list of ClassificationPrediction objects\n",
    "        \"\"\"\n",
    "        # in some cases output can be returned as a list of dictionaries, while dictionary is expected.\n",
    "        # We need handle it inside extract prediction\n",
    "        prediction = self._extract_predictions(raw, frame_meta)[self.output_blob]\n",
    "\n",
    "        # define container to store a batch of predictions as independent entities\n",
    "        result = []\n",
    "        # go through batch dementions and extract results for specific image\n",
    "        for identifier, output in zip(identifiers, prediction):\n",
    "            # depending on the task output representation can be different and has it's own parameters\n",
    "            # for classification, identifier and class probabilities are required\n",
    "            single_prediction = ClassificationPrediction(identifier, output)\n",
    "            result.append(single_prediction)\n",
    "\n",
    "        # return list of prediction representations\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v8EgDaE0yxtH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting open_model_zoo/tools/accuracy_checker/accuracy_checker/adapters/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile open_model_zoo/tools/accuracy_checker/accuracy_checker/adapters/__init__.py\n",
    "\"\"\"\n",
    "Copyright (c) 2019-2020 Intel Corporation\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\"\n",
    "\n",
    "from .adapter import Adapter, AdapterField, create_adapter\n",
    "\n",
    "from .action_recognition import ActionDetection\n",
    "from .text_detection import (\n",
    "    TextDetectionAdapter,\n",
    "    TextProposalsDetectionAdapter,\n",
    "    EASTTextDetectionAdapter\n",
    ")\n",
    "\n",
    "from .text_recognition import (\n",
    "    BeamSearchDecoder,\n",
    "    CTCGreedySearchDecoder,\n",
    "    LPRAdapter\n",
    ")\n",
    "\n",
    "from .image_processing import (\n",
    "    ImageProcessingAdapter, SuperResolutionAdapter, MultiSuperResolutionAdapter, SuperResolutionYUV\n",
    ")\n",
    "from .attributes_recognition import (\n",
    "    HeadPoseEstimatorAdapter,\n",
    "    VehicleAttributesRecognitionAdapter,\n",
    "    PersonAttributesAdapter,\n",
    "    AgeGenderAdapter,\n",
    "    LandmarksRegressionAdapter,\n",
    "    GazeEstimationAdapter,\n",
    "    PRNetAdapter\n",
    ")\n",
    "\n",
    "from .reidentification import ReidAdapter\n",
    "from .detection import (\n",
    "    TFObjectDetectionAPIAdapter,\n",
    "    MTCNNPAdapter,\n",
    "    RetinaNetAdapter,\n",
    "    ClassAgnosticDetectionAdapter,\n",
    "    FaceBoxesAdapter,\n",
    "    FaceDetectionAdapter,\n",
    "    FaceDetectionRefinementAdapter\n",
    ")\n",
    "from .detection_person_vehicle import PersonVehicleDetectionAdapter\n",
    "from .ssd import SSDAdapter, PyTorchSSDDecoder, FacePersonAdapter, SSDAdapterMxNet, SSDONNXAdapter\n",
    "from .retinaface import RetinaFaceAdapter\n",
    "from .yolo import TinyYOLOv1Adapter, YoloV2Adapter, YoloV3Adapter\n",
    "from .classification import ClassificationAdapter\n",
    "from .segmentation import SegmentationAdapter, BrainTumorSegmentationAdapter\n",
    "from .pose_estimation import HumanPoseAdapter\n",
    "from .pose_estimation_3d import HumanPose3dAdapter\n",
    "\n",
    "from .dummy_adapters import XML2DetectionAdapter\n",
    "\n",
    "from .hit_ratio import HitRatioAdapter\n",
    "\n",
    "from .mask_rcnn import MaskRCNNAdapter\n",
    "from .mask_rcnn_with_text import MaskRCNNWithTextAdapter\n",
    "\n",
    "from .nlp import MachineTranslationAdapter, QuestionAnsweringAdapter\n",
    "\n",
    "from .centernet import CTDETAdapter\n",
    "\n",
    "from .mono_depth import MonoDepthAdapter\n",
    "\n",
    "from .image_inpainting import ImageInpaintingAdapter\n",
    "from .style_transfer import StyleTransferAdapter\n",
    "\n",
    "from .attribute_classification import AttributeClassificationAdapter\n",
    "\n",
    "from .emotion_recognition_adapter import EmotionRecognitionAdapter\n",
    "\n",
    "__all__ = [\n",
    "    'Adapter',\n",
    "    'AdapterField',\n",
    "    'create_adapter',\n",
    "\n",
    "    'XML2DetectionAdapter',\n",
    "\n",
    "    'ClassificationAdapter',\n",
    "\n",
    "    'TFObjectDetectionAPIAdapter',\n",
    "    'MTCNNPAdapter',\n",
    "    'CTDETAdapter',\n",
    "    'RetinaNetAdapter',\n",
    "    'ClassAgnosticDetectionAdapter',\n",
    "    'RetinaFaceAdapter',\n",
    "    'FaceBoxesAdapter',\n",
    "    'FaceDetectionAdapter',\n",
    "    'FaceDetectionRefinementAdapter',\n",
    "    'PersonVehicleDetectionAdapter',\n",
    "\n",
    "    'SegmentationAdapter',\n",
    "    'BrainTumorSegmentationAdapter',\n",
    "\n",
    "    'ReidAdapter',\n",
    "\n",
    "    'ImageProcessingAdapter',\n",
    "    'SuperResolutionAdapter',\n",
    "    'MultiSuperResolutionAdapter',\n",
    "    'SuperResolutionYUV',\n",
    "\n",
    "    'HeadPoseEstimatorAdapter',\n",
    "    'VehicleAttributesRecognitionAdapter',\n",
    "    'PersonAttributesAdapter',\n",
    "    'AgeGenderAdapter',\n",
    "    'LandmarksRegressionAdapter',\n",
    "    'GazeEstimationAdapter',\n",
    "    'PRNetAdapter',\n",
    "\n",
    "    'TextDetectionAdapter',\n",
    "    'TextProposalsDetectionAdapter',\n",
    "    'EASTTextDetectionAdapter',\n",
    "\n",
    "    'BeamSearchDecoder',\n",
    "    'LPRAdapter',\n",
    "    'CTCGreedySearchDecoder',\n",
    "\n",
    "    'HumanPoseAdapter',\n",
    "    'HumanPose3dAdapter',\n",
    "\n",
    "    'ActionDetection',\n",
    "\n",
    "    'HitRatioAdapter',\n",
    "\n",
    "    'MaskRCNNAdapter',\n",
    "    'MaskRCNNWithTextAdapter',\n",
    "\n",
    "    'MachineTranslationAdapter',\n",
    "    'QuestionAnsweringAdapter',\n",
    "\n",
    "    'MonoDepthAdapter',\n",
    "\n",
    "    'ImageInpaintingAdapter',\n",
    "    'StyleTransferAdapter',\n",
    "\n",
    "    'AttributeClassificationAdapter',\n",
    "    'EmotionRecognitionAdapter',\n",
    "    \n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9GZQV6R1VYi"
   },
   "source": [
    "### Step 4: reinstall Accuracy Checker to apply new changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QR5MJIZgyqyV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./open_model_zoo/tools/accuracy_checker\n",
      "Collecting numpy<1.18,>=1.11 (from accuracy-checker==0.7.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/ae/c9/69096779fd29bf3066e24124e1c88213e40bf9d2eab4786d21948a37c40b/numpy-1.17.5-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting PyYAML (from accuracy-checker==0.7.7)\n",
      "Collecting tqdm (from accuracy-checker==0.7.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/28/7e/281edb5bc3274dfb894d90f4dbacfceaca381c2435ec6187a2c6f329aed7/tqdm-4.48.2-py2.py3-none-any.whl\n",
      "Collecting pillow>=2.6.1 (from accuracy-checker==0.7.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/30/bf/92385b4262178ca22b34f82e0e09c2922eb351fe39f3cc7b8ba9ea555b41/Pillow-7.2.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting scikit-image (from accuracy-checker==0.7.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/0e/ba/53e1bfbdfd0f94514d71502e3acea494a8b4b57c457adbc333ef386485da/scikit_image-0.17.2-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting scikit-learn (from accuracy-checker==0.7.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/5c/a1/273def87037a7fb010512bbc5901c31cfddfca8080bc63b42b26e3cc55b3/scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting yamlloader (from accuracy-checker==0.7.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/93/a2/2f0c2394af1559021703c8cbb1bc7419bb5a94ea6bde0ab8cd1e973bb605/yamlloader-0.5.5-py3-none-any.whl\n",
      "Collecting py-cpuinfo<=4.0 (from accuracy-checker==0.7.7)\n",
      "Collecting shapely (from accuracy-checker==0.7.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/9d/18/557d4f55453fe00f59807b111cc7b39ce53594e13ada88e16738fb4ff7fb/Shapely-1.7.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting nibabel (from accuracy-checker==0.7.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/8b/8c/cf676b9b3cf69164ba0703a9dcb86ed895ab172e09bece4480db4f03fcce/nibabel-3.1.1-py3-none-any.whl\n",
      "Collecting scipy (from accuracy-checker==0.7.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/2b/a8/f4c66eb529bb252d50e83dbf2909c6502e2f857550f22571ed8556f62d95/scipy-1.5.2-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting pydicom (from accuracy-checker==0.7.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/d3/56/342e1f8ce5afe63bf65c23d0b2c1cd5a05600caad1c211c39725d3a4cc56/pydicom-2.0.0-py3-none-any.whl\n",
      "Collecting sentencepiece (from accuracy-checker==0.7.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/68/e5/0366f50a00db181f4b7f3bdc408fc7c4177657f5bf45cb799b79fb4ce15c/sentencepiece-0.1.92-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting tokenizers (from accuracy-checker==0.7.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/e9/ee/fedc3509145ad60fe5b418783f4a4c1b5462a4f0e8c7bbdbda52bdcda486/tokenizers-0.8.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting editdistance (from accuracy-checker==0.7.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/77/67/2b1fe72bdd13ee9ec32b97959d7dfbfcd7c0548081d69aaf8493c1e695f9/editdistance-0.5.3-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting networkx>=2.0 (from scikit-image->accuracy-checker==0.7.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/9b/cd/dc52755d30ba41c60243235460961fc28022e5b6731f16c268667625baea/networkx-2.5-py3-none-any.whl\n",
      "Collecting PyWavelets>=1.1.1 (from scikit-image->accuracy-checker==0.7.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/59/bb/d2b85265ec9fa3c1922210c9393d4cdf7075cc87cce6fe671d7455f80fbc/PyWavelets-1.1.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting tifffile>=2019.7.26 (from scikit-image->accuracy-checker==0.7.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/42/6b/93a8ee61c6fbe20fa9c17928bd3b80484902b7fd454cecaffba42f5052cb/tifffile-2020.9.3-py3-none-any.whl\n",
      "Collecting matplotlib!=3.0.0,>=2.0.0 (from scikit-image->accuracy-checker==0.7.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/96/a7/b6fa244fd8a8814ef9408c8a5a7e4ed0340e232a6f0ce2046b42e50672c0/matplotlib-3.3.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting imageio>=2.3.0 (from scikit-image->accuracy-checker==0.7.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/6e/57/5d899fae74c1752f52869b613a8210a2480e1a69688e65df6cb26117d45d/imageio-2.9.0-py3-none-any.whl\n",
      "Collecting joblib>=0.11 (from scikit-learn->accuracy-checker==0.7.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/51/dd/0e015051b4a27ec5a58b02ab774059f3289a94b0906f880a3f9507e74f38/joblib-0.16.0-py3-none-any.whl\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->accuracy-checker==0.7.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
      "Collecting packaging>=14.3 (from nibabel->accuracy-checker==0.7.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/46/19/c5ab91b1b05cfe63cccd5cfc971db9214c6dd6ced54e33c30d5af1d2bc43/packaging-20.4-py2.py3-none-any.whl\n",
      "Collecting decorator>=4.3.0 (from networkx>=2.0->scikit-image->accuracy-checker==0.7.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/ed/1b/72a1821152d07cf1d8b6fce298aeb06a7eb90f4d6d41acec9861e7cc6df0/decorator-4.4.2-py2.py3-none-any.whl\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 (from matplotlib!=3.0.0,>=2.0.0->scikit-image->accuracy-checker==0.7.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/8a/bb/488841f56197b13700afd5658fc279a2025a39e22449b7cf29864669b15d/pyparsing-2.4.7-py2.py3-none-any.whl\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib!=3.0.0,>=2.0.0->scikit-image->accuracy-checker==0.7.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/ae/23/147de658aabbf968324551ea22c0c13a00284c4ef49a77002e91f79657b7/kiwisolver-1.2.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting python-dateutil>=2.1 (from matplotlib!=3.0.0,>=2.0.0->scikit-image->accuracy-checker==0.7.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\n",
      "Collecting cycler>=0.10 (from matplotlib!=3.0.0,>=2.0.0->scikit-image->accuracy-checker==0.7.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\n",
      "Collecting certifi>=2020.06.20 (from matplotlib!=3.0.0,>=2.0.0->scikit-image->accuracy-checker==0.7.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/5e/c4/6c4fe722df5343c33226f0b4e0bb042e4dc13483228b4718baf286f86d87/certifi-2020.6.20-py2.py3-none-any.whl\n",
      "Collecting six (from packaging>=14.3->nibabel->accuracy-checker==0.7.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
      "Installing collected packages: numpy, PyYAML, tqdm, pillow, decorator, networkx, PyWavelets, tifffile, scipy, pyparsing, kiwisolver, six, python-dateutil, cycler, certifi, matplotlib, imageio, scikit-image, joblib, threadpoolctl, scikit-learn, yamlloader, py-cpuinfo, shapely, packaging, nibabel, pydicom, sentencepiece, tokenizers, editdistance, accuracy-checker\n",
      "  Running setup.py install for accuracy-checker ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed PyWavelets-1.1.1 PyYAML-5.3.1 accuracy-checker-0.7.7 certifi-2020.6.20 cycler-0.10.0 decorator-4.4.2 editdistance-0.5.3 imageio-2.9.0 joblib-0.16.0 kiwisolver-1.2.0 matplotlib-3.3.1 networkx-2.5 nibabel-3.1.1 numpy-1.17.5 packaging-20.4 pillow-7.2.0 py-cpuinfo-4.0.0 pydicom-2.0.0 pyparsing-2.4.7 python-dateutil-2.8.1 scikit-image-0.17.2 scikit-learn-0.23.2 scipy-1.5.2 sentencepiece-0.1.92 shapely-1.7.1 six-1.15.0 threadpoolctl-2.1.0 tifffile-2020.9.3 tokenizers-0.8.1 tqdm-4.48.2 yamlloader-0.5.5\n"
     ]
    }
   ],
   "source": [
    "!pip3 install open_model_zoo/tools/accuracy_checker --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTHONPATH=\"$PWD/open_model_zoo/tools/accuracy_checker:$PYTHONPATH\"\n",
    "#import sys\n",
    "#sys.path.append(\"open_model_zoo/tools/accuracy_checker\")\n",
    "#print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/intel/openvino_2020.4.287/python/python3.6:/opt/intel/openvino_2020.4.287/python/python3:/opt/intel/openvino_2020.4.287/deployment_tools/open_model_zoo/tools/accuracy_checker:/opt/intel/openvino_2020.4.287/deployment_tools/model_optimizer:/opt/intel/openvino_2020.4.287/data_processing/dl_streamer/python:/opt/intel/openvino_2020.4.287/data_processing/gstreamer/lib/python3.6/site-packages:/opt/intel/openvino_2020.4.287/python/python3.6:/opt/intel/openvino_2020.4.287/python/python3:/opt/intel/openvino_2020.4.287/deployment_tools/open_model_zoo/tools/accuracy_checker:/opt/intel/openvino_2020.4.287/deployment_tools/model_optimizer:/opt/intel/openvino_2020.4.287/data_processing/dl_streamer/python:/opt/intel/openvino_2020.4.287/data_processing/gstreamer/lib/python3.6/site-packages:\n"
     ]
    }
   ],
   "source": [
    "!echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YBIxOtj71wHm"
   },
   "source": [
    "### Step 5. Creating POT configuration file and running the quantization.\n",
    "\n",
    "Now, lets use introduced functionality inside POT config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i2xnaJLkeDTt"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/onnx/models/raw/master/vision/body_analysis/emotion_ferplus/model/emotion-ferplus-8.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "33QuuYAROUcx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/home/u41987/My-Notebooks/POT_tutorial/emotion-ferplus-8.onnx\n",
      "\t- Path for generated IR: \t/home/u41987/My-Notebooks/POT_tutorial/.\n",
      "\t- IR output name: \temotion-ferplus-8\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \t[1, 1, 64, 64]\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tFalse\n",
      "\t- Reverse input channels: \tFalse\n",
      "ONNX specific parameters:\n",
      "Model Optimizer version: \t\n",
      "\n",
      "[ SUCCESS ] Generated IR version 10 model.\n",
      "[ SUCCESS ] XML file: /home/u41987/My-Notebooks/POT_tutorial/./emotion-ferplus-8.xml\n",
      "[ SUCCESS ] BIN file: /home/u41987/My-Notebooks/POT_tutorial/./emotion-ferplus-8.bin\n",
      "[ SUCCESS ] Total execution time: 6.51 seconds. \n",
      "[ SUCCESS ] Memory consumed: 299 MB. \n"
     ]
    }
   ],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo.py --input_model emotion-ferplus-8.onnx --input_shape \"[1, 1, 64, 64]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t-15_NbmeWqJ"
   },
   "outputs": [],
   "source": [
    "cat advanced_materials/emotion_recognition.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FF9WWxHa2k_G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04:53:08 accuracy_checker WARNING: /usr/local/lib/python3.6/dist-packages/onnxruntime/capi/_pybind_state.py:13: UserWarning: Cannot load onnxruntime.capi. Error: '/usr/local/lib/python3.6/dist-packages/onnxruntime/capi/onnxruntime_pybind11_state.so: undefined symbol: _ZN15InferenceEngine4Core11LoadNetworkENS_10CNNNetworkERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKSt3mapIS7_S7_St4lessIS7_ESaISt4pairIS8_S7_EEE'\n",
      "  warnings.warn(\"Cannot load onnxruntime.capi. Error: '{0}'\".format(str(e)))\n",
      "\n",
      "04:53:08 accuracy_checker WARNING: /opt/intel/openvino_2020.4.287/deployment_tools/model_optimizer/extensions/back/ReverseInputChannels.py:134: DeprecationWarning: invalid escape sequence \\ \n",
      "  \"\"\"\n",
      "\n",
      "04:53:08 accuracy_checker WARNING: /opt/intel/openvino_2020.4.287/deployment_tools/model_optimizer/extensions/back/ReverseInputChannels.py:194: DeprecationWarning: invalid escape sequence \\ \n",
      "  \"\"\"\n",
      "\n",
      "04:53:08 accuracy_checker WARNING: /opt/intel/openvino_2020.4.287/deployment_tools/model_optimizer/extensions/back/ReverseInputChannels.py:297: DeprecationWarning: invalid escape sequence \\ \n",
      "  \"\"\"\n",
      "\n",
      "04:53:09 accuracy_checker WARNING: /opt/intel/openvino_2020.4.287/deployment_tools/model_optimizer/mo/front/tf/graph_utils.py:178: DeprecationWarning: invalid escape sequence \\*\n",
      "  \"\"\"\n",
      "\n",
      "04:53:09 accuracy_checker WARNING: /opt/intel/openvino_2020.4.287/deployment_tools/model_optimizer/extensions/back/compress_quantized_weights.py:88: DeprecationWarning: invalid escape sequence \\ \n",
      "  \"\"\"\n",
      "\n",
      "INFO:app.run:Output log dir: ./results/emotion_recognition_FP32/2020-09-07_04-53-09\n",
      "INFO:app.run:Creating pipeline:\n",
      " ===========================================================================\n",
      "IE version: 2.1.2020.4.0-359-21e092122f4-releases/2020/4\n",
      "Loaded CPU plugin version:\n",
      "    CPU - MKLDNNPlugin: 2.1.2020.4.0-359-21e092122f4-releases/2020/4\n",
      "INFO:compression.pipeline.pipeline:Evaluation of generated model\n",
      "INFO:compression.engines.ac_engine:Start inference on the whole dataset\n",
      "Total dataset size: 3589\n",
      "1000 / 3589 processed in 2.329s\n",
      "2000 / 3589 processed in 2.276s\n",
      "3000 / 3589 processed in 2.274s\n",
      "3589 objects processed in 8.173 seconds\n",
      "INFO:compression.engines.ac_engine:Inference finished\n",
      "INFO:app.run:accuracy@top1              : 0.07996656450264698\n",
      "INFO:app.run:accuracy@top5              : 0.5366397325160212\n"
     ]
    }
   ],
   "source": [
    "!pot -c emotion_recognition.json -e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uBJr4__q2vOA"
   },
   "source": [
    "It works! Now we can modify `compression` section content to find quantized model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7RPUoYg03M2j"
   },
   "source": [
    "## Case 2: Yolo V3\n",
    "\n",
    "Yolo V3 is complicated from the configuration perspective:\n",
    "1. labeles start with 0, while in majority of detection models uses dataset with background\n",
    "2. Model evaluation requires to know some details about output layers (anchors, cell size, e.t.c.)\n",
    "3. Model has several outputs\n",
    "\n",
    "Information about all necessary parameters can be found in Accuracy Checker readme.\n",
    "Beside that, there is also predefined config for model inside OMZ. It can be used as template for your own model evaluation.\n",
    "AccuracyChecker config for yolo_v3 can be found [here](https://github.com/opencv/open_model_zoo/blob/master/tools/accuracy_checker/configs/yolo-v3-tf.yml)\n",
    "\n",
    "So you do not need to modify Accuracy Checker for getting correct configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kdMlD6iyPWPx"
   },
   "source": [
    "## Case 3: DCSCN\n",
    "\n",
    "Superresolution network with 5D input tensor, which represents sequence of 5 frames.\n",
    "\n",
    "At the first glance, it looks like as an unusual case where we need to create new entities for AC, but inside [annotation conversion guide](https://github.com/opencv/open_model_zoo/blob/master/tools/accuracy_checker/accuracy_checker/annotation_converters/README.md) we can find the following converter description:\n",
    "\n",
    "`multi_frame_super_resolution` - converts dataset for super resolution task with multiple input frames usage.\n",
    "* `data_dir` - path to folder, where images in low and high resolution are located.\n",
    "* `lr_suffix` - low resolution file name's suffix (default lr).\n",
    "* `hr_suffix` - high resolution file name's suffix (default hr).\n",
    "* `annotation_loader` - the library which will be used for ground truth image reading. Supported: opencv, pillow (Optional. Default value is pillow). Note, color space of an image depends on the loader (OpenCV uses BGR, Pillow uses RGB for image reading).\n",
    "* `number_input_frames` - the number of input frames per inference.\n",
    "\n",
    "It sound very simmilar on our use case, does not it?\n",
    "It can be adopted for our use case:\n",
    "* handle more complicated image names\n",
    "* use midle frame as gt reference\n",
    "\n",
    "More details provided in OMZ [pull request 1083](https://github.com/opencv/open_model_zoo/pull/1083)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "POT_training_ADVANCED",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (OpenVINO)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
